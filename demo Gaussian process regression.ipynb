{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:e475f65f9265baf46ac66724762e73f4c6a4d603a61a422cd4d3b76d9f918f68"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gauss process regression \n",
      "Links to more information:\n",
      "* Carl Rasmussen and Chris Williams book on GPs: http://www.gaussianprocess.org/gpml/chapters/RW.pdf\n",
      "* general page on GPs in Machine learning: http://www.gaussianprocess.org\n",
      "* David Duvenaud's page about kernels: http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html\n",
      "* Philipp Hennig's animation paper: http://mlss.tuebingen.mpg.de/Hennig_2013_Animating_Samples_from_Gaussian_Distributions.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define mean functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mzero(a) = 0.0    # zero mean function\n",
      "mone(a)  = 1.0    # one mean function (useful to take off the mean, see Mauna Loa data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define basic kernel functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function kse(a, b, khp)\n",
      "    # squared exponential (actually exponentiated square), Eq. 4.9 in Rasmussen/Williams\n",
      "    sigma2 = khp[1]^2   # output variance\n",
      "    ell2   = khp[2]^2   # lengthscale squared\n",
      "    r      = a - b\n",
      "    return sigma2 * exp(-dot(r,r)/(2*ell2))\n",
      "end\n",
      "function klin(a, b, khp)\n",
      "    # linear, following Duvenaud's kernel page (link see above)\n",
      "    sigma2b = khp[1]^2  # variance determining value at zero\n",
      "    sigma2v = khp[2]^2  # ???\n",
      "    c       = khp[3]    # shift to determine location for zero\n",
      "    return sigma2b + sigma2v * dot(a-c, b-c)\n",
      "end\n",
      "function krq(a,b,khp)\n",
      "    # rational quadratic, Eq. 4.19 in Rasmussen/Williams\n",
      "    sigma2 = khp[1]^2   # output variance\n",
      "    ell2   = khp[2]^2   # lengthscale squared\n",
      "    alpha  = khp[3]^2   # relative weighting of large-scale vs small-scale variations\n",
      "    r      = a - b\n",
      "    return sigma2 * (1 + dot(r,r)/(2*alpha*ell2))^(-alpha)\n",
      "end\n",
      "function kper(a, b, khp)\n",
      "    # periodic kernel, Eq. 4.31 in Rasmussen/Williams\n",
      "    sigma2 = khp[1]^2   # output variance\n",
      "    ell2   = khp[2]^2   # lengthscale squared\n",
      "    p      = khp[3]^2   # period\n",
      "    return sigma2 * exp(-2 * sin(pi * p * (a-b)[1]/2)^2 / ell2)\n",
      "end\n",
      "function kmat(a, b, khp) \n",
      "    # Matern class, Eq. 4.14 in Rasmussen/Williams\n",
      "    sigma2 = khp[1]^2   # output variance\n",
      "    ell    = khp[2]^2   # lengthscale\n",
      "    nu     = khp[3]^2   # for nu>k we get at least k-times differentiability\n",
      "    r      = abs(a-b)[1]\n",
      "    z      = sqrt(2*nu)*r/ell\n",
      "    return sigma2 * ((2^(1-nu)) / gamma(nu)) * (z^nu) * besselk(nu, z)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To obtain a mean vector and a kernel matrix we have to \n",
      "apply mean and kernel functions to each vector in a matrix of locations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernelmat(X1,X2,k,khp) = Float64[k(X1[:,i],X2[:,j],khp) for i in 1:size(X1,2), j in 1:size(X2,2)]\n",
      "meanvec(X,m)           = Float64[m(X[:,i]) for i in 1:size(X,2)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a single set of locations we need for visualizations the diagonal of the kernel matrix.  For convenience we already take the square root to obtain the vector of standard deviations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stdvec(X,k,khp) = Float64[sqrt(k(X[:,i],X[:,i],khp)) for i in 1:size(X,2)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Define a Gaussian process\n",
      "A GP is defined by its mean function and covariance function.  Additionally we store the values of the hyperparameters of the covariance function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable GP\n",
      "    m       # mean function\n",
      "    k       # covariance function\n",
      "    khp     # hyperparameters of the covariance function\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Get the data\n",
      "The data consists of locations $X$ and values at those locations $y$.  The number of columns of $X$ must equal the length of $y$ (we do not check that)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "immutable Data\n",
      "    X   # matrix of locations (each column is location)\n",
      "    y   # vector of values\n",
      "end\n",
      "using Gadfly\n",
      "function layerData(d::Data, sigma2=0.0)\n",
      "    if sigma2 == 0.0\n",
      "        return layer(x=d.X, y=d.y, Geom.point, \n",
      "                     Theme(default_color=color(\"gray\"), default_point_size=0.4mm))\n",
      "    else\n",
      "        sigma = sqrt(sigma2)\n",
      "        return layer(x=d.X, y=d.y, ymin=d.y-sigma, ymax=d.y+sigma, Geom.point, Geom.errorbar, \n",
      "                     Theme(default_color=color(\"gray\"), default_point_size=0.4mm))\n",
      "    end\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pick a dataset (and the kernel functions) by setting a value for `dataset`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add new datasets by adding a new block enclosed in an `if` statement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if dataset == 1\n",
      "    # simple 1D data\n",
      "    fname = \"TrainingSet1D.csv\"\n",
      "    Draw  = readdlm(fname, ',')\n",
      "    D     = Data(Draw[:,1]', Draw[:,2])\n",
      "    # set mean and kernel function\n",
      "    # STEP 1: set mean and kernel function\n",
      "    m(a)  = mean(D.y) * mone(a)\n",
      "    k(a,b,khp) = klin(a,b,khp[1:3]) + kse(a,b,khp[4:5]) + kper(a,b,khp[6:8])\n",
      "    # STEP 2: set the number of hyperparameters appropriately (add one for the noise variance)\n",
      "    nhp   = 9\n",
      "    hp = [ -14.7691, 0.0597952, -6.6571, 10.8033, 5.99427, 1.45272, -0.326692, -0.460548, -0.485503]\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if dataset == 2\n",
      "    # GBR in EURO (almost daily)\n",
      "    fname = \"GBR-EUR.txt\"\n",
      "    Draw  = readdlm(\"gbr-eur.txt\")\n",
      "    Draw  = Draw[1:40:end,:]   # no longer daily...\n",
      "    datestr2float(d) = float(d[1:4]) + float(d[6:7])/12.0 + float(d[9:10])/365.0\n",
      "    D = Data(float(map(datestr2float, Draw[:,1]))', float(1./Draw[:,2]))    \n",
      "    # STEP 1: set mean and kernel function\n",
      "    m(a)     = mzero(a)\n",
      "    k(a,b,khp) = krq(a,b,khp[1:3]) + klin(a,b,khp[4:6]) + kper(a,b,khp[7:9])\n",
      "    # STEP 2: set the number of hyperparameters appropriately (PLUS ONE FOR THE NOISE VARIANCE)\n",
      "    nhp   = 10    # ADD ONE FOR THE NOISE VARIANCE\n",
      "    hp = [0.37788, 2.06071, -0.0909124, 1.47744, -2.58826e-5, -1.74576, 1.16072, 0.805948, -0.14992, -0.0203695]\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if dataset == 3\n",
      "# Mauna Loa CO2 dataset\n",
      "    fname = \"maunaloa.co2\"\n",
      "    Draw = readdlm(fname)[16:66,1:13]   # chop out table\n",
      "    if false\n",
      "        Draw[1,2] = Draw[2,2]  # fill in missing data\n",
      "        Draw[1,3] = Draw[2,3]\n",
      "        Draw[7,3] = Draw[8,3]\n",
      "        Draw[7,4] = Draw[8,4]\n",
      "        Draw[7,5] = Draw[8,5]\n",
      "    end\n",
      "    beg = 1      # ignore the older data\n",
      "    ende = 550   # end like Rasmussen\n",
      "    D = Data(float(Draw[:,1] .+ (0:11)'/12)'[beg:ende]', float(Draw[:,2:13]'[beg:ende]))\n",
      "    if true\n",
      "        # remove missing data, where value is -99.99, see original file\n",
      "        D = Data(D.X[1,[3:73, 77:end]], D.y[[3:73, 77:end]])\n",
      "    end\n",
      "    # STEP 1: set mean and kernel function\n",
      "    m(a)     = mean(D.y) * mone(a)\n",
      "    k(a,b,khp) = kse(a,b,khp[1:2]) + kse(a,b,khp[3:4])*kper(a,b,khp[5:7]) + krq(a,b,khp[8:10]) + kse(a,b,khp[11:12])\n",
      "    # STEP 2: set the number of hyperparameters appropriately (add one for the noise variance)\n",
      "    nhp   = 13\n",
      "    # hyperparameters from Rasmussen\n",
      "    hp = [66.0, 67.0, 2.4, 90.0, 1, 1.3, sqrt(2), 0.66, 1.2, sqrt(0.78), 0.18, 1.6, 0.19]\n",
      "    # hyperparameters after running the optimizer\n",
      "    #hp = [-273.783, 82.3786, 37.0953, -81.85, 0.0475119, 0.516356, 1.00019, 67.6745, 85.1728, 0.00348616, 118.741, -30.2149, 0.212153]    \n",
      "    #hp = [0.170338, 0.146736, 0.393137, 11.3821, -4.94044, -1.30842, 1.41497, 2.09715, -2.56222, -0.577909, -30.7039, 10.1871, 0.196506]\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if dataset == 4\n",
      "    # your choice!\n",
      "    error(\"REPLACE THIS LINE WITH CODE THAT LOADS DATA\")\n",
      "    # STEP 1: set mean and kernel function\n",
      "    m     = mzero\n",
      "    k(a,b,khp) = kse(a,b,khp)\n",
      "    # STEP 2: set the number of hyperparameters appropriately (add one for the noise variance)\n",
      "    nhp   = 3\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(layerData(D))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding the hyperparameters\n",
      "Given the data we can find good hyperparameter by maximizing the log marginal likelihood:\n",
      "$$ \\log p(y|X,\\theta) = -\\frac{1}{2} (y-m_X)^T (K_{XX} + \\sigma^2 I)^{-1} (y-m_X) - \\frac{1}{2} \\log |K_{XX}+\\sigma^2 I| - \\frac{n}{2} \\log 2\\pi $$\n",
      "Note that we subtract the mean vector from the observations, since we do not assume that the mean function is zero, which is often done in many textbooks.  Note that $\\theta$ appears on the right-hand-side as well, since $K_{XX}$ and $\\sigma^2$ depend on it.\n",
      "\n",
      "The expression above comes from the fact that assuming a Gaussian process with hyperparameters $\\theta$ the observations $y$ at locations $X$ are Gaussian distributed as follows:\n",
      "$$ y|X,\\theta \\sim N(m_X, K_{XX} + \\sigma^2 I) $$\n",
      "where $m_X$ is the mean vector, i.e. the mean function evaluated at the locations $X$, similarly the covariance matrix $K_{XX}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function logmarginallikelihood(g::GP, sigma2, d::Data)\n",
      "    X = d.X                       # locations of the data\n",
      "    m = meanvec(X, g.m)           # mean vector\n",
      "    K = kernelmat(X, X, g.k, g.khp)  # cov matrix\n",
      "    n = length(d.y)               # number of data points\n",
      "    Z = K + sigma2*eye(n)         # the second term comes from the measurement noise\n",
      "    y = d.y - m                   # subtract the mean vector from y\n",
      "    # note: \n",
      "    #    dot(y, Z\\y) == (y' * inv(Z) * y)[1]\n",
      "    # where the left-hand-side might be faster because of the backslash operator.\n",
      "    logdetZ = 2*sum(log(diag(chol(Z))))\n",
      "    return -0.5*dot(y, Z\\y) - 0.5*logdetZ - log(2*pi)*n/2\n",
      "    # where we used the following trick to reliably calculate logdetZ\n",
      "    # (B -> (2*sum(log(diag(chol(B)))), log(det(B)))) ((x->x*x')(randn(5,5)))\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before running the next cell make sure `m`, `k` and `nhp` have been chosen."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Optim\n",
      "g(khp) = GP(m, k, khp)                # function that defines a GP (including hyperparameters)\n",
      "f(hp)  = -logmarginallikelihood(g(hp[1:end-1]), hp[end]^2, D)   # define the objective function\n",
      "hp0    = randn(nhp)                  # random starting point\n",
      "res    = optimize(f, hp0)            # find the minimum\n",
      "hp     = res.minimum                 # the optimal hyperparameters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The value of the marginal loglikelihood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f(hp)          # value of marginal loglikelihood"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After fitting the hyperparameter we store the noise variance and create the GP prior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma2  = hp[end]^2             # noise variance\n",
      "khp     = hp[1:end-1]           # kernel hyperparameters\n",
      "priorGP = g(khp)                # define the prior GP (with fixed hyperparameters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### From prior to posterior GP \n",
      "Starting with a GP prior over functions\n",
      "$$ f \\sim GP(m_0, k_0) $$\n",
      "with mean function $m_0$ and covariance function $k_0$, the posterior after seeing data $X$ and $y$ is also a GP where the mean function and covariance function have been updated:\n",
      "$$ f|X,y \\sim GP(m_n, k_n) $$\n",
      "$$ m_n(a) = m_0(a) + k_0(X,a)^T  (K_{XX}+\\sigma^2 I)^{-1}  (y - m_X)$$\n",
      "$$ k_n(a,b) = k_0(a,b) - k_0(X,a)^T  (K_{XX}+\\sigma^2 I)^{-1}  k_0(X, b) $$\n",
      "where we used the mean vector \n",
      "$$ m_X = m_0(X) $$\n",
      "and the covariance matrix\n",
      "$$ K_{XX} = k_0(X,X) $$\n",
      "Note that $k_0(X,a)$ is a vector containing the similarities between $a$ and the dataset locations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function updateGP(g::GP, sigma2, d::Data)\n",
      "    khp      = g.khp                          # hyperparameters of cov function\n",
      "    X        = d.X                            # locations\n",
      "    mX       = meanvec(X, g.m)                # prior mean vector\n",
      "    kXX      = kernelmat(X, X, g.k, khp)      # prior cov matrix\n",
      "    kX(x)    = kernelmat(X, x, g.k, khp)      # how similar is the test location to the data locations\n",
      "    y        = d.y - mX                       # subtract mean from the observations\n",
      "    n        = length(y)                      # number of data points\n",
      "    Z        = kXX + sigma2*eye(n)            # the second term comes from the measurement noise\n",
      "    postm(a) = g.m(a) + dot(kX(a), Z\\y)       # posterior mean function\n",
      "    postk(a,b,khp) = g.k(a,b,khp) - dot(kX(a), Z\\kX(b)) # posterior cov function\n",
      "    return GP(postm, postk, g.khp)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posteriorGP = updateGP(priorGP, sigma2, D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Visualize GPs\n",
      "The easiest is to plot the mean function plus/minus the standard deviation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "function Xrange(d::Data; margin=0.5)\n",
      "    minX = minimum(d.X)\n",
      "    maxX = maximum(d.X)\n",
      "    minX = minX - margin*(maxX-minX)    # extend to the left\n",
      "    maxX = maxX + margin*(maxX-minX)    # extend to the right\n",
      "    return (minX, maxX)\n",
      "end  \n",
      "function layerGP(g::GP, minX, maxX; n=100, margin=0.5)\n",
      "    X    = linspace(minX, maxX, n)'\n",
      "    mX   = meanvec(X, g.m)\n",
      "    stdX = stdvec(X, g.k, g.khp)\n",
      "    return layer(x=X, y=mX, ymin=mX-stdX, ymax=mX+stdX, Geom.line, Geom.ribbon)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(minX, maxX) = Xrange(D)\n",
      "n = 100\n",
      "if dataset == 3\n",
      "    minX = 1960\n",
      "    maxX = 2025\n",
      "    n    = 300\n",
      "end\n",
      "size(D.X, 1) == 1 || error(\"locations must be one-dimensional for plotting\")\n",
      "plot(layerData(D), layerGP(posteriorGP, minX, maxX, n=n), Guide.title(\"posterior GP\"))\n",
      "#plot(layerGP(priorGP, minX, maxX), Guide.title(\"prior GP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posteriorGP.khp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How to sample from a GP with `randn`\n",
      "First let's see how we could sample from a multivariate Gaussian:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "dd    = 5                         # dimensionality\n",
      "mu    = randn(dd)                 # a random mean\n",
      "Sigma = (A->A*A')(randn(dd,dd))   # a random covariance matrix, A->A*A' ensures pos. def.\n",
      "rand(MvNormal(mu, Sigma), 1)      # sample a single vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This allows us to draw a random sample from a GP and to plot it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n   = 100                           # number of locations\n",
      "(minX, maxX) = Xrange(D)\n",
      "X   = linspace(minX, maxX, n)'      # locations for function plotting\n",
      "g2  = priorGP                       # a GP\n",
      "mX  = meanvec(X, g2.m)              # the mean vector\n",
      "kXX = kernelmat(X, X, g2.k, g2.khp) # the covariance matrix\n",
      "kXX = kXX + 1e-10 * eye(n)          # ensure positive definiteness of kXX (required by MvNormal)\n",
      ";"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "J = 5                            # how many samples\n",
      "Y = rand(MvNormal(mX, kXX), J)   # draw function values\n",
      "plot(layerGP(g2, minX, maxX),\n",
      "     [layer(x=X, y=Y[:,j], Geom.line, Theme(default_color=color(\"lightgray\"))) for j=1:J]..., \n",
      "     Guide.title(\"$k random samples from a GP\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we don't really draw a random function, but instead we draw random function values at fixed locations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cool animations of the GP distribution \n",
      "Philipp Hennig came up with a cool way to visualize GP distributions.  This implementation is based on the following paper:\n",
      "* http://mlss.tuebingen.mpg.de/Hennig_2013_Animating_Samples_from_Gaussian_Distributions.pdf\n",
      "\n",
      "First of all, we need to understand how to draw a multivariate Gaussian sample without the `distribution` package.\n",
      "We need that to create a random ellipse according to a given Gaussian distribution.  So here is the recipe to sample from a Gaussian distribution for given mean and variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dd = 5                                   # dimensionality\n",
      "x  = randn(dd, 1)                        # x ~ N(zeros(dd), eye(dd))\n",
      "y  = mu .+ chol(Sigma+1e-10*eye(dd))'*x; # y ~ N(mu, Sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where `chol` calculates the Choleski decomposition of a matrix, which acts like a matrix square root, i.e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "R = chol(Sigma)\n",
      "norm(R'*R - Sigma)       # a small number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we added a small number to the diagonal for stability, more precisely to ensure full rank (which is required by `chol`)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sample a random ellipse."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "circ(n) = hcat(cos(2*pi*[1:n]/n), sin(2*pi*[1:n]/n))'   # circle in 2D\n",
      "randrot(d) = svd(randn(d,2))[1]                         # random rotation from 2D to dD\n",
      "randrotcirc(d,n) = randrot(d) * circ(n) * sqrt(d)       # random circle in dD, the sqrt(d) increases the radius for higher dims\n",
      "randnellipse(n, mu, Sigma) = mu .+ chol(Sigma+1e-8*eye(length(mu)))' * randrotcirc(length(mu),n)\n",
      "                                                        # shifted random ellipse in dD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To understand the `sqrt(d)` term, consider the norm of a random sample of a `d` dimensional Gaussian, which is `sqrt(d)` (is there an easy proof for this result?)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dd = 1000\n",
      "(sqrt(dd), norm(randn(dd)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#using Interact, Reactive\n",
      "#function animateGP(g::GP, minX, maxX; J=1, n=20, fps=20.0)\n",
      "##    X    = linspace(minX, maxX, 100)'\n",
      "#    mX   = meanvec(X, g.m)              # the mean vector\n",
      "#    kXX  = kernelmat(X, X, g.k, g.khp)  # the covariance matrix\n",
      "#    Y    = [randnellipse(n, mX, kXX) for j=1:J]   # sample J ellipses\n",
      "#    maxY = maximum(Y[1])              # should consider all, however this is easier to implement\n",
      "#    minY = minimum(Y[1])\n",
      "#    @manipulate for play=true, i = foldl((acc, vote) -> (acc+1>n ? 1 : acc+1), 1, fpswhen(play,fps))\n",
      "#        plot([layer(x=X, y=Y[j][:,i], Geom.line, Theme(default_color=color(\"lightgray\"))) for j=1:J]..., \n",
      "#             layerGP(g, minX, maxX),\n",
      "#             Scale.y_continuous(minvalue=minY, maxvalue=maxY))\n",
      "#    end\n",
      "#end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#(minX, maxX) = Xrange(D)\n",
      "#animateGP(posteriorGP, minX, maxX, J=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}