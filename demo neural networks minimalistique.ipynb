{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:5eb49f98292f7b0a9b80d000a9a9510947fcf639015db39b420dd2912d8acce5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Minimalistic neural network implementation\n",
      "A neural network is a list of computational bricks.  A computational brick might have parameters.  The bricks transfrom input into output (feedforward).  But also they can update their parameters if passed an error (backpropagation)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Abstract brick\n",
      "Subtypes of AbstractBrick must implement:\n",
      "    forward(b, input)                  # apply the brick to input and create output\n",
      "    backprop!(b, err, input, output)   # update the parameters and return the transformed error\n",
      "\n",
      "Notes\n",
      "* backprop! passes also 'input' and 'output' for convenience\n",
      "* 'err' has the same size as the 'output'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract AbstractBrick                 # an operation between layers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Linear brick\n",
      "\n",
      "The forward processing is a matrix-vector multiplication:\n",
      "$$ f(x) = W  x $$\n",
      "where $W$ is an $m\\times n$ matrix.\n",
      "The error that gets passed is a $m$ dimensional vector $v$.  For the differential we get\n",
      "$$ v^T d (Wx) = v^T W dx + tr v^T dW x = v^T W dx + \\text{tr} x v^T dW $$\n",
      "Thus the derivative of the loss function $\\ell$ wrt to $W$ is:\n",
      "$$ D_W \\ell = x v^T $$\n",
      "and the error that is propagated further is:\n",
      "$$ D_x \\ell = v^T W$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type LinearBrick <: AbstractBrick            # fully connected operation\n",
      "    W        # weight matrix\n",
      "    lr       # learning rate\n",
      "end\n",
      "forward(b::LinearBrick, input) = b.W * input\n",
      "backprop!(b::LinearBrick, derr, input, output) = begin\n",
      "    Wtpderr = b.W' * derr                 # transform the error before the update\n",
      "    b.W = b.W - b.lr * (derr * input')    # update the weights\n",
      "    return Wtpderr                        # back propagate the transformed error\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bias brick"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type BiasBrick <: AbstractBrick\n",
      "    bias     # bias\n",
      "    lr       # learning rate\n",
      "end\n",
      "forward(b::BiasBrick, input) = input + b.bias\n",
      "backprop!(b::BiasBrick, derr, input, output) = begin\n",
      "    b.bias = b.bias - b.lr * derr         # update the bias\n",
      "    return derr                           # back propagate the transformed error\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Subsampling brick"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type SubsamplingBrick <: AbstractBrick\n",
      "    strides\n",
      "end\n",
      "forward(b::SubsamplingBrick, input) = begin\n",
      "    idx = [1:b.strides[i]:size(input,i) for i in 1:length(b.strides)]\n",
      "    return input[idx...]\n",
      "end\n",
      "backprop!(b::SubsamplingBrick, derr, input, output) = begin\n",
      "    derrout = zeros(size(input))\n",
      "    idx = [1:b.strides[i]:size(input,i) for i in 1:length(b.strides)]\n",
      "    derrout[idx] = derr\n",
      "    return derrout\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Vectorizing brick"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type VecBrick <: AbstractBrick end\n",
      "forward(b::VecBrick, input) = vec(input)\n",
      "backprop!(b::VecBrick, derr, input, output) = reshape(derr, size(input))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convolutional brick\n",
      "Consider a matrix $x$ as input and a single filter $f$.  Then the output is the valid convolution of $x$ and $f$, possibly subsampled with some stride."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type ConvolutionBrick <: AbstractBrick\n",
      "    filters  # array of filters, e.g. [5,5,10] for ten 5x5 filters\n",
      "    lr       # learning rate\n",
      "end\n",
      "feedforward(b::ConvolutionBrick, input) = begin\n",
      "    error(\"not yet\")\n",
      "end\n",
      "backprop!(b::ConvolutionBrick, derr, input, output) = begin\n",
      "    error(\"not yet\")\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sigmoidal brick\n",
      "The forward processing is calculating component-wise the sigmoidal function\n",
      "$$ \\sigma(t) = \\frac{1}{1+e^{-t}} $$\n",
      "The differential of the sigmoid has a special form:\n",
      "$$ d \\sigma(t) = \\sigma(t)(1-\\sigma(t)) dt $$\n",
      "Note that the sigmoidal unit operates component-wise, i.e. for a vector $x$ we get,\n",
      "$$ \\sigma(x) = [\\sigma(x_1), \\ldots, \\sigma(x_n)]$$\n",
      "Similarly for the differential:\n",
      "$$ d\\sigma(x) = \\text{Diag}(\\sigma(x) \\odot (1-\\sigma(x))) dx$$\n",
      "There are no parameters to update.  But the error $v$ has to be propagated correctly.  Consider a vector $x$\n",
      "$$ v^T d\\sigma(x) = v^T \\text{Diag}(\\sigma(x) \\odot (1-\\sigma(x))) dx$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type SigmoidBrick <: AbstractBrick end\n",
      "forward(b::SigmoidBrick, input) = 1 ./ (1 .+ exp(-input))\n",
      "backprop!(b::SigmoidBrick, derr, input, output) = derr .* (output.*(1-output))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Rectified linear brick\n",
      "The rectified linear brick calculates\n",
      "$$ f(x) = \\max(x, 0) $$\n",
      "Thus its derivative is `diagm(x .>= 0)`, which is a matrix with ones and zeros along the diagonal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type ReLUBrick <: AbstractBrick end\n",
      "forward(b::ReLUBrick, input) = max(input, 0)\n",
      "backprop!(b::ReLUBrick, derr, input, output) = derr .* float64(input .>= 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tanh brick\n",
      "The tangens hyperbolicus brick calculates\n",
      "$$ f(x) = \\tanh(x) $$\n",
      "Thus their derivative is `diagm(x .>= 0)`, which is a matrix with ones and zeros along the diagonal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type TanhBrick <: AbstractBrick end\n",
      "forward(b::TanhBrick, input) = tanh(input)\n",
      "backprop!(b::TanhBrick, derr, input, output) = derr .* (1 - output.^2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Dropout brick\n",
      "Randomly set inputs to zero.  We need to store which were set to zero for the back prop."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type DropoutBrick <: AbstractBrick\n",
      "    p                 # probability to zero a node\n",
      "    mask              # which stores the random selection until backprop!\n",
      "    DropoutBrick(p=0.5) = new(p, [])\n",
      "end\n",
      "forward(b::DropoutBrick, input) = begin\n",
      "    b.mask = (rand(size(input)) .<= b.p)\n",
      "    return input .* b.mask\n",
      "end\n",
      "backprop!(b::DropoutBrick, derr, input, output) = derr .* b.mask"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Simple test code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test code (checks only for size missmatch bugs)\n",
      "n = 4\n",
      "m = 5\n",
      "sizein = (n,)\n",
      "input  = randn(sizein)\n",
      "for b in [ReLUBrick(), SigmoidBrick(), TanhBrick(), LinearBrick(randn(m,n), 1e-1), DropoutBrick()]\n",
      "    output = forward(b, input)\n",
      "    derr   = randn(size(output))\n",
      "    backprop!(b, derr, input, output)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Loss functions\n",
      "For now we define only squared loss and its derivative:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss(ytrue, y)  = 0.5 * vecnorm( ytrue - y )^2     # squared loss, should work for arbitrary arrays\n",
      "dloss(ytrue, y) = ytrue - y                        # derivative of squared loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Neural networks\n",
      "Neural networks are lists of bricks.  Since in our implementation the bricks do not store the values of the nodes, we also store the values in the neural networks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type NeuralNetwork\n",
      "    bricks::Vector{AbstractBrick}  # vector of bricks\n",
      "    values                         # the list of values at each layer (i.e. between the bricks)\n",
      "    function NeuralNetwork(bricks::Vector{AbstractBrick})\n",
      "        values = Array(Any, length(bricks)+1)\n",
      "        return new(bricks, values)\n",
      "    end\n",
      "end\n",
      "\n",
      "function NeuralNetwork(arch::Vector{Int64}; lr=1e-1, activation=SigmoidBrick)\n",
      "    larch = length(arch)\n",
      "    bricks = Array(AbstractBrick, 2*larch-3)\n",
      "    j     = 1\n",
      "    for i in 2:larch\n",
      "        (m,n) = (arch[i], arch[i-1])           # output and input size\n",
      "        W = sqrt(n) * randn(m, n)              # standard deviation sqrt(n)\n",
      "        bricks[j] = LinearBrick(W, lr/n)       # learning rate normalized by input layer size\n",
      "        j = j + 1\n",
      "        if i < length(arch)\n",
      "            bricks[j] = activation()\n",
      "            j = j + 1\n",
      "        end\n",
      "    end\n",
      "    return NeuralNetwork(bricks)\n",
      "end\n",
      "\n",
      "function forward!(nn::NeuralNetwork, input)\n",
      "    nn.values[1] = input\n",
      "    for i in 1:length(nn.bricks)\n",
      "        nn.values[i+1] = forward(nn.bricks[i], nn.values[i])\n",
      "    end\n",
      "    return nn.values[end]      # the output are the values at the last layer\n",
      "end\n",
      "\n",
      "function backprop!(nn::NeuralNetwork, input, output)\n",
      "    foutput = forward!(nn, input)         # fills all layers\n",
      "    derr = dloss(foutput, output)         # calculates loss\n",
      "    for i in length(nn.bricks):-1:1\n",
      "        input  = nn.values[i]\n",
      "        output = nn.values[i+1]\n",
      "        derr   = backprop!(nn.bricks[i], derr, input, output)  \n",
      "                                          # update the parameters of the units along the way\n",
      "    end\n",
      "    return foutput                        # the output of the forward\n",
      "end\n",
      "\n",
      "function train!(nn::NeuralNetwork, X, y)\n",
      "    n = size(X,1)                # number of training examples\n",
      "    errs = Array(Float64, n)     # the training errors\n",
      "    for i in 1:n                 # stochastic gradient descent\n",
      "        errs[i] = loss(y[i], backprop!(nn,vec(X[i,:]),y[i]))\n",
      "    end\n",
      "    return errs\n",
      "end\n",
      "\n",
      "apply!(nn::NeuralNetwork, X) = [forward!(nn, vec(X[i,:])) for i in 1:size(X,1)]\n",
      "\n",
      "meanloss(nn, X, y) = mean([loss(ytrue, yest) for (ytrue, yest) in zip(y, apply!(nn, X))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Run on iris data set for debugging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using RDatasets\n",
      "iris = dataset(\"datasets\", \"iris\")         # using RDatasets\n",
      "classes = unique(iris[5])\n",
      "posclass = classes[2]    # PICK YOUR CLASS HERE\n",
      "X      = array(iris[1:4])\n",
      "C      = {\"virginica\"=>1e0, \"setosa\"=>1e0, \"versicolor\"=>1e0}[posclass]    # to store \"good\" values for C per class\n",
      "y      = [label == posclass ? 1.0 : 0.0 for label in iris[5]]  # convert to +1.0 and 0.0\n",
      "srand(17)        # set the random function to a fixed initial state for reproducibility\n",
      "train  = randbool(size(iris, 1))       # create a random split in training and test set\n",
      "Xtrain = X[ train, :]\n",
      "ytrain = y[ train]\n",
      "Xtest  = X[~train, :]\n",
      "ytest  = y[~train]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Convenience functions for plotting\n",
      "using Gadfly\n",
      "import Gadfly.plot\n",
      "plot(y::Vector) = plot(x=[1:length(y)], y=y)\n",
      "plot(x::Vector, y::Vector) = plot(x=[1:length(x)], y=x, color=y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork([4, 10, 1], lr=1e-1)     # define neural network\n",
      "err = train!(nn, [Xtrain;Xtrain], [ytrain, ytrain])\n",
      "plot(err)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork([4, 10, 1], lr=1e-1, activation=SigmoidBrick)     # define neural network\n",
      "ntrain   = size(Xtrain, 1)           # number of training examples\n",
      "errtest  = zeros(ntrain)\n",
      "errtrain = zeros(ntrain)\n",
      "for i in 1:size(Xtrain, 1)\n",
      "    train!(nn, Xtrain[i,:], y[i])\n",
      "    errtest[i]  = meanloss(nn, Xtest, ytest)\n",
      "    errtrain[i] = meanloss(nn, Xtrain, ytrain)\n",
      "end\n",
      "plot(x=[1:ntrain, 1:ntrain], \n",
      "     y = [errtrain, errtest], \n",
      "     color=[i<=ntrain ? \"train\" : \"test\" for i in 1:(2*ntrain)],\n",
      "     Scale.y_log10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}