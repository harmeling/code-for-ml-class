{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:229366747cb64a592dfadc31d694117aff2323bc798367b2b43adad749ca670c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preliminaries\n",
      "\n",
      "It is quite convenient to visualize matrices as little images.  For this we define `imagesc`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Winston\n",
      "imagesc(randn(5,5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using PyPlot\n",
      "imagesc(A) = imshow(A)\n",
      "imagesc(randn(5,5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Images, Color\n",
      "## syntactic sugar for Images.jl\n",
      "scale0_1(A::Array) = (A - minimum(A))./(maximum(A)-minimum(A))\n",
      "scale1_255(A::Array) = 254 * scale0_1(A) + 1\n",
      "cmblues   = colormap(\"Blues\", 255)\n",
      "cmoranges = colormap(\"Oranges\", 255)\n",
      "imagesc(x, cm=cmblues) = ImageCmap(iround(scale1_255(x')), cm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To test our implementation we will apply it to some classic dataset which is included in Julia's package `RDatasets`.  \n",
      "Note that the following command will issue lots of warnings which you can ignore (if you want to know why there are warning look at this thread https://github.com/JuliaLang/julia/issues/6190).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using RDatasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, we define some easy to use plotting functions based on Julia's package `Gadfly.jl`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Gadfly\n",
      "import Gadfly.plot\n",
      "plot(y::Vector) = plot(x=[1:length(y)], y=y)\n",
      "plotclasses(X, y, title) = plot(x=X[:,1], y=X[:,2], color=[label==1 ? posclass : \"~$posclass\" for label in y], Guide.title(title), Guide.xlabel(\"x[1]\"), Guide.ylabel(\"x[2]\"))\n",
      "plot(x::Vector, y::Vector) = plot(x=[1:length(x)], y=x, color=y)\n",
      "function plot(A::Matrix, y::Vector)\n",
      "    d = size(A, 2)    # number of features\n",
      "    vstack([hstack([plot(x=A[:,i], y=A[:,j], color=y, Guide.xlabel(\"x[$i]\"), Guide.ylabel(\"x[$j]\")) for i=1:d]...) for j=1:d]...)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Kernel functions and kernel matrix\n",
      "\n",
      "Next we define some popular kernel functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kgauss(a,b,sigma2) = exp(-0.5*dot(a-b,a-b)/sigma2)   # Gaussian kernel\n",
      "kpoly(a,b,p,c)     = 0 #MISSING#                     # polynomial kernel\n",
      "klinear(a,b)       = kpoly(a,b,1,0)                  # linear kernel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a data set `X` where each row is a data point and given a kernel function, we can calculate the Gram matrix or kernel matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernelmat(k,X1,X2) = [k(vec(X1[i,:]),vec(X2[j,:]))::Float64 for i in 1:size(X1,1), j in 1:size(X2,1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is interesting to visualize the kernel matrix.  Here is an example how to use `kernelmat` and how to look that the kernel matrix.  What happens if you change the parameter of the Gaussian kernel?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k(a,b) = kgauss(a,b,1e0)\n",
      "Xclass1 = randn(100,3)                  # locations of class one\n",
      "Xclass2 = randn(50,3)+5                 # locations of class two\n",
      "Xboth   = [Xclass1, Xclass2]\n",
      "K = kernelmat(k, Xboth, Xboth)\n",
      "imagesc(K)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementation of a support vector machine\n",
      "\n",
      "The support vector machine for the non-separable case solves the following optimization problem:\n",
      "$$\n",
      "\\text{min}_\\alpha\\;\\;\\;\\; \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j \\;x_i^T x_j\\\\\n",
      "\\text{s.t.}\\;\\;\\;\\;  0 \\le\\alpha\\le C \\text{ and } \\sum_{i=1}^n \\alpha_i y_i = 0\n",
      "$$\n",
      "Note that the expression in the objective function can be rewritten as follows:\n",
      "$$\n",
      "\\newcommand{\\dotp}[2]{\\langle {#1}, {#2}\\rangle}% bracket notation for the dot product\n",
      "\\sum_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j \\;x_i^T x_j \n",
      "= \\sum_{i,j=1}^n \\alpha_i \\alpha_j y_i y_j \\; \\dotp{x_i}{x_j} \n",
      "= \\mathtt{alpha'*diagm(y)*X*X'*diagm(y)*alpha}\n",
      "$$\n",
      "Note furthermore that the weight vector $w$ is a linear combination of $x_i$, \n",
      "$$ \n",
      "w = \\sum_{i=1}^n \\alpha_i y_i x_i\n",
      "$$\n",
      "However, if we kernelize the SVM we can not always calculate $w$.  Luckily we don't have to.  Instead storing $w$ and $b$ as the output of the SVM, we store `Xtrain`, `ytrain`, `alpha` and `b`.  This can be used in `svmpredictraw` to calculate the output of the SVM, following the formula:\n",
      "$$\n",
      "f(x) = \\sum_{i=1}^n \\alpha_i y_i x_i^T x + b \n",
      "= \\sum_{i=1}^n \\alpha_i y_i \\dotp{x_i}{x} + b \n",
      "= \\mathtt{X * Xtrain' * diagm(ytrain) * alpha + b}\n",
      "$$\n",
      "We see that, again, the data locations $x_i$ appear only inside dot products, in this as $x_i^T x$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the support vector machine\n",
      "using QP\n",
      "type SVMfit\n",
      "    Xtrain    # training locations\n",
      "    ytrain    # training labels\n",
      "    alpha     # dual variables\n",
      "    b         # threshold\n",
      "    k         # kernel function\n",
      "end\n",
      "function svmtrain(X, y, C, k = klinear)\n",
      "    # inputs:\n",
      "    #    X  contains training locations as rows\n",
      "    #    y  vector of training labels +1 or -1\n",
      "    #    C  regularization constant\n",
      "    #    k  a kernel function, defaults to linear kernel function\n",
      "    # outputs:\n",
      "    #    w  weight vector\n",
      "    #    b  threshold\n",
      "    n  = size(X, 1)     # number of data points\n",
      "    if n != length(y) error(\"size missmatch\") end\n",
      "    K  = X*X'\n",
      "    Q  = diagm(y)*K*diagm(y)    # faster is probably   y .* K .* y'\n",
      "    (alpha, val, flag) = quadprog(Q, -ones(n), lb=zeros(n), ub=C*ones(n), Aeq=y', beq=zeros(1), x0=0*rand(n))\n",
      "    Xw = K * diagm(y) * alpha   # faster is probably   (K .* y) * alpha\n",
      "    sp = sortperm(Xw)\n",
      "    b  = -mean(Xw[sp[min(n, [0,1]+indmin(cumsum(ytrain[sp])))]])\n",
      "    return SVMfit(X, y, alpha, b, k)\n",
      "end\n",
      "svmpredictraw(svm, X) = X * svm.Xtrain' * diagm(svm.ytrain) * svm.alpha + svm.b\n",
      "function svmpredict(svm , X)\n",
      "    # inputs:\n",
      "    #    svm     an SVMfit created by svmtrain\n",
      "    #    X       contains test locations as rows\n",
      "    # outputs:\n",
      "    #    y       vector of test labels +1 or -1\n",
      "    return 2 * (svmpredictraw(svm, X) .> 0.0) - 1\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Simple classification example\n",
      "\n",
      "We will test our SVM implementation on Fisher's Iris flower dataset (see http://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
      "\n",
      "Visualize the dataset by plotting the coordinates against each other."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = dataset(\"datasets\", \"iris\")         # using RDatasets\n",
      "set_default_plot_size(25cm, 20cm)\n",
      "classes = unique(iris[5])\n",
      "plot(array(iris[:,1:4]), array(iris[:,5]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will classify one of the flower types against the other two.  We plot the two classes for the first two coordinates to get some impression.  However, note that even if a class might not be separable in the first two components, it might be separable by combining the third or forth.\n",
      "\n",
      "Note that each row in `X` is a data point, same for `Xtrain` and `Xtest`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posclass = classes[2]    # PICK YOUR CLASS HERE\n",
      "X      = array(iris[1:4])\n",
      "C      = {\"virginica\"=>1e0, \"setosa\"=>1e0, \"versicolor\"=>1e0}[posclass]    # to store \"good\" values for C per class\n",
      "y      = [label == posclass ? 1 : -1 for label in iris[5]]  # convert to +1 and -1 (see johnmyleswhite SVM.jl)\n",
      "srand(17)        # set the random function to a fixed initial state for reproducibility\n",
      "train  = randbool(size(iris, 1))       # create a random split in training and test set\n",
      "Xtrain = X[ train, :]\n",
      "ytrain = y[ train]\n",
      "Xtest  = X[~train, :]\n",
      "ytest  = y[~train]\n",
      "set_default_plot_size(25cm, 10cm)\n",
      "hstack(plotclasses(Xtrain, ytrain, \"training\"), \n",
      "       plotclasses(Xtest, ytest, \"test\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's train an SVM on the training data and test it on the test data.  Note that the raw output of the SVM should be positive for one class and negative for the other classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train an svm and apply it to the test data\n",
      "k(a,b)       = klinear(a,b)\n",
      "svm          = svmtrain(Xtrain, ytrain, C, k)     # using the C defined per class, put other values here to play around\n",
      "ytestest     = svmpredict(svm, Xtest)\n",
      "err(y,ytrue) = mean(abs(y-ytrue)/2)               # the error will be between 0 and 1\n",
      "println(\"test error == $(err(ytestest,ytest))\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check whether the positive examples are above the axis\n",
      "plot(svmpredictraw(svm, Xtest), [label==1?\"pos\":\"neg\" for label in ytest])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}